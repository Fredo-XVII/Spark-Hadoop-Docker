{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 56A0-1A81\n",
      "\n",
      " Directory of C:\\Users\\z001c9v\\Documents\\Spark-Hadoop-Docker\n",
      "\n",
      "08/29/2016  03:06 PM    <DIR>          .\n",
      "08/29/2016  03:06 PM    <DIR>          ..\n",
      "08/29/2016  03:00 PM    <DIR>          .ipynb_checkpoints\n",
      "08/29/2016  02:55 PM                77 README.md\n",
      "08/29/2016  03:06 PM               940 Untitled.ipynb\n",
      "               2 File(s)          1,017 bytes\n",
      "               3 Dir(s)  336,120,811,520 bytes free\n",
      "('TMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('COMPUTERNAME', '9XK2R72')\n",
      "('PROCESSOR_LEVEL', '6')\n",
      "('LIB', ';C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('USERDOMAIN', 'DHC')\n",
      "('SPARK_HOME', 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\')\n",
      "('PSMODULEPATH', 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\PowerShell\\\\Modules\\\\')\n",
      "('COMMONPROGRAMFILES', 'C:\\\\Program Files\\\\Common Files')\n",
      "('PROCESSOR_IDENTIFIER', 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel')\n",
      "('VBOX_MSI_INSTALL_PATH', 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\')\n",
      "('PROGRAMFILES', 'C:\\\\Program Files')\n",
      "('PROCESSOR_REVISION', '5e03')\n",
      "('DATACONNECTORLIBPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('SYSTEMROOT', 'C:\\\\WINDOWS')\n",
      "('CLICOLOR', '1')\n",
      "('PROGRAMFILES(X86)', 'C:\\\\Program Files (x86)')\n",
      "('COMSPEC', 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe')\n",
      "('TERM', 'xterm-color')\n",
      "('DOCKER_TOOLBOX_INSTALL_PATH', 'C:\\\\Apps\\\\Docker Toolbox')\n",
      "('WINDOWS_TRACING_LOGFILE', 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log')\n",
      "('DB2INSTANCE', 'DB2')\n",
      "('PROCESSOR_ARCHITECTURE', 'AMD64')\n",
      "('ALLUSERSPROFILE', 'C:\\\\ProgramData')\n",
      "('SCCMNORDCDOWNLOAD', '1')\n",
      "('LOCALAPPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local')\n",
      "('HOMEPATH', '\\\\')\n",
      "('USERDOMAIN_ROAMINGPROFILE', 'DHC')\n",
      "('JAVA_HOME', 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\')\n",
      "('JPY_INTERRUPT_EVENT', '1032')\n",
      "('PROGRAMW6432', 'C:\\\\Program Files')\n",
      "('USERNAME', 'z001c9v')\n",
      "('COPLIB', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('LOGONSERVER', '\\\\\\\\TCTTSDCC09P')\n",
      "('PROMPT', '$P$G')\n",
      "('WINDOWS_TRACING_FLAGS', '3')\n",
      "('JPY_PARENT_PID', '1016')\n",
      "('PROGRAMDATA', 'C:\\\\ProgramData')\n",
      "('PYTHONPATH', 'C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;')\n",
      "('CLASSPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\tdgeospatial.jar;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\jmsam.jar;.;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2java.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\sqlj.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc_license_cu.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\bin;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\common.jar')\n",
      "('PATH', 'C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Apps\\\\R\\\\Rtools\\\\bin;C:\\\\Apps\\\\R\\\\Rtools\\\\gcc-4.6.3\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Teradata Performance Monitor Object 15.00\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\msg;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\ODBC Driver for Teradata\\\\Lib;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files\\\\SAS94\\\\x86\\\\Secure\\\\ccme4;C:\\\\Program Files\\\\SAS94\\\\Secure\\\\ccme4;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\tdwallet\\\\nt-i386\\\\;C:\\\\Program Files (x86)\\\\Microsoft Application Virtualization Client;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin\\\\;C:\\\\WINDOWS\\\\SysWOW64\\\\;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\SysWOW64\\\\Wbem;C:\\\\WINDOWS\\\\SysWOW64\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program Files (x86)\\\\AdminStudio\\\\9.5\\\\Common;C:\\\\PROGRA~1\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Program Files\\\\ibm\\\\gsk8\\\\lib64;C:\\\\Program Files (x86)\\\\ibm\\\\gsk8\\\\lib;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\ManagementStudio\\\\;C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\Common7\\\\IDE\\\\PrivateAssemblies\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Apps\\\\Anaconda2;C:\\\\Apps\\\\Anaconda2\\\\Scripts;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda3;C:\\\\Apps\\\\Anaconda3\\\\Scripts;C:\\\\Apps\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;C:\\\\Apps\\\\Docker Toolbox;C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\;C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2;C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\rodeo\\\\app-2.1.1\\\\bin')\n",
      "('HOMESHARE', '\\\\\\\\corp.target.com\\\\dfsroot\\\\HomeDirs\\\\HQ02\\\\59808964')\n",
      "('GIT_PAGER', 'cat')\n",
      "('UATDATA', 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77')\n",
      "('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC')\n",
      "('INCLUDE', 'C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\INCLUDE;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('SESSIONNAME', 'Console')\n",
      "('FP_NO_HOST_CHECK', 'NO')\n",
      "('WINDIR', 'C:\\\\WINDOWS')\n",
      "('TEMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('HOMEDRIVE', 'H:')\n",
      "('OS', 'Windows_NT')\n",
      "('SYSTEMDRIVE', 'C:')\n",
      "('PUBLIC', 'C:\\\\Users\\\\Public')\n",
      "('NUMBER_OF_PROCESSORS', '8')\n",
      "('APPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Roaming')\n",
      "('USERDNSDOMAIN', 'CORP.TARGET.COM')\n",
      "('IBMDB2', 'C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN')\n",
      "('PAGER', 'cat')\n",
      "('COMMONPROGRAMW6432', 'C:\\\\Program Files\\\\Common Files')\n",
      "('HADOOP_HOME', 'C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2')\n",
      "('COMMONPROGRAMFILES(X86)', 'C:\\\\Program Files (x86)\\\\Common Files')\n",
      "('IPY_INTERRUPT_EVENT', '1032')\n",
      "('USERPROFILE', 'C:\\\\Users\\\\z001c9v')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.11 :: Anaconda 4.0.0 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!dir\n",
    "%pwd\n",
    "!python --version\n",
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8014d44c0992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yarn-client'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('yarn-client')\n",
    "conf.setAppName('HadoopDockerTest')\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Initialization: DOCKER \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda2\\;C:\\Apps\\Anaconda2\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.ConnectException: Call From 9XK2R72/192.168.56.1 to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy9.getNewApplication(Unknown Source)\r\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:221)\r\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy10.getNewApplication(Unknown Source)\r\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:219)\r\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:227)\r\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:157)\r\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:149)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:500)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.ConnectException: Connection refused: no further information\r\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\r\n\t... 28 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cc9d19ece0e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmaster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"yarn-client\"\u001b[0m \u001b[1;31m# 192.168.99.100:32770\"  #spark://172.17.0.2:50010\"  #\"yarn-client\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapp_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.env(/usr/local/hadoop/etc/hadoop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 115\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1183\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\Apache\\spark-2.0.0\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.ConnectException: Call From 9XK2R72/192.168.56.1 to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat sun.reflect.GeneratedConstructorAccessor12.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n\tat com.sun.proxy.$Proxy9.getNewApplication(Unknown Source)\r\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:221)\r\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy10.getNewApplication(Unknown Source)\r\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:219)\r\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:227)\r\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:157)\r\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:149)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:500)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:236)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.ConnectException: Connection refused: no further information\r\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\r\n\t... 28 more\r\n"
     ]
    }
   ],
   "source": [
    "# Initiate Spark on Local:  ** Do not run this on Docker **\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python\\\\lib\\\\py4j-0.10.1-src.zip')) #Note, this needs to be in PYTHONPATH env variable.\n",
    "\n",
    "\n",
    "print(os.environ['PYTHONPATH'])\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"HadoopDockerTest\"\n",
    "#master = \"local[*]\" #--local mode\n",
    "master = \"yarn-client\" # 192.168.99.100:32770\"  #spark://172.17.0.2:50010\"  #\"yarn-client\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master) #.env(/usr/local/hadoop/etc/hadoop)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "#sc.sql(\"SET -v\").show(n=200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "# contributor license agreements.  See the NOTICE file distributed with\n",
      "# this work for additional information regarding copyright ownership.\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "# (the \"License\"); you may not use this file except in compliance with\n",
      "# the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "# This file is sourced when running various Spark programs.\n",
      "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
      "\n",
      "# Options read when launching programs locally with\n",
      "# ./bin/run-example or ./bin/spark-submit\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program\n",
      "# - SPARK_CLASSPATH, default classpath entries to append\n",
      "\n",
      "# Options read by executors and drivers running inside the cluster\n",
      "# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node\n",
      "# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program\n",
      "# - SPARK_CLASSPATH, default classpath entries to append\n",
      "# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data\n",
      "# - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos\n",
      "\n",
      "# Options read in YARN client mode\n",
      "# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files\n",
      "# - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)\n",
      "# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).\n",
      "# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)\n",
      "# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)\n",
      "\n",
      "# Options for the daemons used in the standalone deploy mode\n",
      "# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname\n",
      "# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master\n",
      "# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")\n",
      "# - SPARK_WORKER_CORES, to set the number of cores to use on this machine\n",
      "# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\n",
      "# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker\n",
      "# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node\n",
      "# - SPARK_WORKER_DIR, to set the working directory of worker processes\n",
      "# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")\n",
      "# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).\n",
      "# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")\n",
      "# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. \"-Dx=y\")\n",
      "# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")\n",
      "# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers\n",
      "\n",
      "# Generic options for the daemons used in the standalone deploy mode\n",
      "# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)\n",
      "# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)\n",
      "# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)\n",
      "# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)\n",
      "# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)\n"
     ]
    }
   ],
   "source": [
    "#!cd /usr/local/sbin\n",
    "!cat C:\\Apps\\Apache\\spark-2.0.0\\conf\\spark-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
