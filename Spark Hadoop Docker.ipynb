{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 56A0-1A81\n",
      "\n",
      " Directory of C:\\Users\\z001c9v\\Documents\\Spark-Hadoop-Docker\n",
      "\n",
      "08/29/2016  03:06 PM    <DIR>          .\n",
      "08/29/2016  03:06 PM    <DIR>          ..\n",
      "08/29/2016  03:00 PM    <DIR>          .ipynb_checkpoints\n",
      "08/29/2016  02:55 PM                77 README.md\n",
      "08/29/2016  03:06 PM               940 Untitled.ipynb\n",
      "               2 File(s)          1,017 bytes\n",
      "               3 Dir(s)  336,120,811,520 bytes free\n",
      "('TMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('COMPUTERNAME', '9XK2R72')\n",
      "('PROCESSOR_LEVEL', '6')\n",
      "('LIB', ';C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('USERDOMAIN', 'DHC')\n",
      "('SPARK_HOME', 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\')\n",
      "('PSMODULEPATH', 'C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\PowerShell\\\\Modules\\\\')\n",
      "('COMMONPROGRAMFILES', 'C:\\\\Program Files\\\\Common Files')\n",
      "('PROCESSOR_IDENTIFIER', 'Intel64 Family 6 Model 94 Stepping 3, GenuineIntel')\n",
      "('VBOX_MSI_INSTALL_PATH', 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\')\n",
      "('PROGRAMFILES', 'C:\\\\Program Files')\n",
      "('PROCESSOR_REVISION', '5e03')\n",
      "('DATACONNECTORLIBPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('SYSTEMROOT', 'C:\\\\WINDOWS')\n",
      "('CLICOLOR', '1')\n",
      "('PROGRAMFILES(X86)', 'C:\\\\Program Files (x86)')\n",
      "('COMSPEC', 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe')\n",
      "('TERM', 'xterm-color')\n",
      "('DOCKER_TOOLBOX_INSTALL_PATH', 'C:\\\\Apps\\\\Docker Toolbox')\n",
      "('WINDOWS_TRACING_LOGFILE', 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log')\n",
      "('DB2INSTANCE', 'DB2')\n",
      "('PROCESSOR_ARCHITECTURE', 'AMD64')\n",
      "('ALLUSERSPROFILE', 'C:\\\\ProgramData')\n",
      "('SCCMNORDCDOWNLOAD', '1')\n",
      "('LOCALAPPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local')\n",
      "('HOMEPATH', '\\\\')\n",
      "('USERDOMAIN_ROAMINGPROFILE', 'DHC')\n",
      "('JAVA_HOME', 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\')\n",
      "('JPY_INTERRUPT_EVENT', '1032')\n",
      "('PROGRAMW6432', 'C:\\\\Program Files')\n",
      "('USERNAME', 'z001c9v')\n",
      "('COPLIB', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\')\n",
      "('LOGONSERVER', '\\\\\\\\TCTTSDCC09P')\n",
      "('PROMPT', '$P$G')\n",
      "('WINDOWS_TRACING_FLAGS', '3')\n",
      "('JPY_PARENT_PID', '1016')\n",
      "('PROGRAMDATA', 'C:\\\\ProgramData')\n",
      "('PYTHONPATH', 'C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;')\n",
      "('CLASSPATH', 'C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\tdgeospatial.jar;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\jmsam.jar;.;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2java.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\sqlj.zip;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\db2jcc_license_cu.jar;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\bin;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\java\\\\common.jar')\n",
      "('PATH', 'C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\Apps\\\\R\\\\Rtools\\\\bin;C:\\\\Apps\\\\R\\\\Rtools\\\\gcc-4.6.3\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Teradata Performance Monitor Object 15.00\\\\;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\client\\\\15.00\\\\Teradata Parallel Transporter\\\\msg;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\ODBC Driver for Teradata\\\\Lib;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\bin\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Program Files\\\\SAS94\\\\x86\\\\Secure\\\\ccme4;C:\\\\Program Files\\\\SAS94\\\\Secure\\\\ccme4;C:\\\\Program Files (x86)\\\\Teradata\\\\Client\\\\15.00\\\\tdwallet\\\\nt-i386\\\\;C:\\\\Program Files (x86)\\\\Microsoft Application Virtualization Client;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin\\\\;C:\\\\WINDOWS\\\\SysWOW64\\\\;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\SysWOW64\\\\Wbem;C:\\\\WINDOWS\\\\SysWOW64\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program Files (x86)\\\\AdminStudio\\\\9.5\\\\Common;C:\\\\PROGRA~1\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Program Files\\\\ibm\\\\gsk8\\\\lib64;C:\\\\Program Files (x86)\\\\ibm\\\\gsk8\\\\lib;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\FUNCTION;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\SAMPLES\\\\REPL;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\Tools\\\\Binn\\\\ManagementStudio\\\\;C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\Common7\\\\IDE\\\\PrivateAssemblies\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\110\\\\DTS\\\\Binn\\\\;C:\\\\Apps\\\\Anaconda2;C:\\\\Apps\\\\Anaconda2\\\\Scripts;C:\\\\Apps\\\\Anaconda2\\\\Library\\\\bin;C:\\\\Apps\\\\Anaconda3;C:\\\\Apps\\\\Anaconda3\\\\Scripts;C:\\\\Apps\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Program Files\\\\Java\\\\jre1.8.0_101\\\\\\\\bin;C:\\\\Apps\\\\Anaconda2\\\\;C:\\\\Apps\\\\Anaconda2\\\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;C:\\\\Apps\\\\Docker Toolbox;C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN;C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\;C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2;C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\rodeo\\\\app-2.1.1\\\\bin')\n",
      "('HOMESHARE', '\\\\\\\\corp.target.com\\\\dfsroot\\\\HomeDirs\\\\HQ02\\\\59808964')\n",
      "('GIT_PAGER', 'cat')\n",
      "('UATDATA', 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77')\n",
      "('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC')\n",
      "('INCLUDE', 'C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\INCLUDE;C:\\\\PROGRA~1\\\\IBM\\\\SQLLIB\\\\LIB')\n",
      "('SESSIONNAME', 'Console')\n",
      "('FP_NO_HOST_CHECK', 'NO')\n",
      "('WINDIR', 'C:\\\\WINDOWS')\n",
      "('TEMP', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Local\\\\Temp')\n",
      "('HOMEDRIVE', 'H:')\n",
      "('OS', 'Windows_NT')\n",
      "('SYSTEMDRIVE', 'C:')\n",
      "('PUBLIC', 'C:\\\\Users\\\\Public')\n",
      "('NUMBER_OF_PROCESSORS', '8')\n",
      "('APPDATA', 'C:\\\\Users\\\\z001c9v\\\\AppData\\\\Roaming')\n",
      "('USERDNSDOMAIN', 'CORP.TARGET.COM')\n",
      "('IBMDB2', 'C:\\\\Program Files (x86)\\\\IBM\\\\SQLLIB\\\\BIN')\n",
      "('PAGER', 'cat')\n",
      "('COMMONPROGRAMW6432', 'C:\\\\Program Files\\\\Common Files')\n",
      "('HADOOP_HOME', 'C:\\\\Apps\\\\Apache\\\\hadoop-2.7.2')\n",
      "('COMMONPROGRAMFILES(X86)', 'C:\\\\Program Files (x86)\\\\Common Files')\n",
      "('IPY_INTERRUPT_EVENT', '1032')\n",
      "('USERPROFILE', 'C:\\\\Users\\\\z001c9v')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.11 :: Anaconda 4.0.0 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "!dir\n",
    "%pwd\n",
    "!python --version\n",
    "import os\n",
    "\n",
    "for key, value in os.environ.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8014d44c0992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yarn-client'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('yarn-client')\n",
    "conf.setAppName('HadoopDockerTest')\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark Initialization: DOCKER \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda2\\;C:\\Apps\\Anaconda2\\Scripts;$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.1-src.zip;\n",
      "<pyspark.context.SparkContext object at 0x000000000652CF98>\n",
      "<pyspark.sql.context.SQLContext object at 0x0000000004009470>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'toDebugString' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0f50458b756e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Import some libraries to work with dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toDebugString' is not defined"
     ]
    }
   ],
   "source": [
    "# Initiate Spark on Local:  ** Do not run this on Docker **\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = 'C:\\\\Apps\\\\Apache\\\\spark-2.0.0\\\\'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python\\\\lib\\\\py4j-0.10.1-src.zip')) #Note, this needs to be in PYTHONPATH env variable.\n",
    "\n",
    "\n",
    "print(os.environ['PYTHONPATH'])\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"HadoopDockerTest\"\n",
    "master = \"local[*]\" #--local mode\n",
    "#master = \"spark://192.168.99.100:32770\"  #\"yarn-client\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9053a94b948f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sc.stop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SET -v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'sql'"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "sc.sql(\"SET -v\").show(n=200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "#!cd /usr/local/sbin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
